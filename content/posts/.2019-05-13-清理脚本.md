脚本有2 种用法，添加 install 参数表示安装（会指导用户添加计划任务），其余均通过管道读取配置；所以需要通过如下命令运行：

```shell
cat clean.conf | ./clean.sh
```

正常状态会获得如下输出

```shell
======================================================================
 Backup UNKNOWN - Tue May 14 03:50:33 UTC 2019
======================================================================
Path /var/log/test1/comp1 of test1 is NOT EXIST
Path /var/log/test1/comp2 of test1 is NOT EXIST
Path /var/log/test2 of test2 is NOT EXIST
Backup hdfs in /var/log/hdfs
 - archive /var/log/hdfs/asdfas.log.1 to ./logs/2019-05-13/hdfs.tar
   + asdfas.log.1
 - delete /var/log/hdfs/asdfas.log.1
Backup hdfs END
Path ../log/yar of hdfs is NOT EXIST
Backup yarn in /var/log/yarn
Backup yarn END
Backup hadoop in /var/log/hadoop
 - move /var/log/hadoop/hadoop-2014-15-12.tar.gz to ./logs/2019-05-13
 - move /var/log/hadoop/hadoop-2014-15-12.tgz to ./logs/2019-05-13
 - archive /var/log/hadoop/adfa.log_2014_15_12 to ./logs/2019-05-13/hadoop.tar
   + adfa.log_2014_15_12
 - delete /var/log/hadoop/adfa.log_2014_15_12
Backup hadoop END
```

输出包含以下几个部分：

* 标题：脚本说明，版本，执行日期
* 处理的每个路径
  * 文件夹是否存在
  * 使用的处理方式
    * 对于归档压缩的操作会输出对应的文件信息，以及是创建压缩档案（`archive`），还是添加文件（`update`）
    * 对于删除会分为删除和清空，分别是 `delete` 和 `clean`
  * 每个组件结束会有结束的输出标记 `Backup xxx END`

#### 脚本可配置常量

`SCRIPT_COMMIT_SHA` : 脚本的对应的版本信息，会在输出中显示

`BACKUP_DIR`: 备份和定时清理的目标目录

#### 整体流程

##### 安装指引

1. 获取脚本路径，生成 `crontab` 表达式，并提示给用户
2. 打开 `crontab` 编辑页面，等待用户输入
3. 检查结果是否包含当前脚本，存在即判定成功

##### 备份

1. 通过输入流，读取用户传递进来的内容
2. 逐行拆分成 `compName` 和 `paths` 部分，调用 `backup_comp_logs` 方法
   1. 加载对应组件的配置 `$compName.conf`，没有则使用 `default.conf` 作为配置
   2. 解析配置文件，逐行处理为 `action` 和 `exps` 部分
   3. 检查配置的组件目录是否存在
   4. 存在则 `find_and_do`，传递上面获取的日志路径，`action`，`exps`
      1. 枚举 `exps` 数组，使用`find` 查找目标日志路径下所有符合要求的文件路径
      2. 执行 `$action` 方法，传递必要的参数
         1. `action` 方法为预先定义的 `delete`, `move`, `archive` 方法

##### 清理历史归档

1. 在上述步骤完成之后，执行清理方法 `clear_logs`，并接受一个清理日期作为参数（默认 30天之前）

2. 从全局常量 `BACKUP_DIR` 中获取目录列表，内容为 `YYYY-MM-DD` 格式，日期满足字典序

3. 循环上述列表，直至目录名称大于或等于参数传递进来的日期。

   

#### 函数即参数说明

`delete (path)` 

递归删除目录下的文件，使用 `fuser` 检查占用情况，未占用会被删除，而占用的只会填写空字符串覆盖其内容

`move(path)`

移动参数中的路径到`BACKUP_DIR/$date` 下

`archive(path)` 

会检查当前组件的归档目录是否存在，存在则直接添加，不存在创建新归档文件（`compName.tar`）

`find_and_do(action, path, ...regs)`

查找 `path` 下符合 `regs` 的所有文件，并调用 `action` 方法。

`backup_comp_logs(date, compName, ...paths)`

备份组件，`date` 是为了生成目标归档目录，`compName` 用于生成归档文件名，`...paths` 是通过 `clean.conf` 传递进来的组件对应的多个目录。

`clear_logs(date)`

遍历`BACKUP_DIR` 并删除早于 `date` 的所有文件，使用注意字典序，并不会检测文件名是否符合日期的标准。

`do_main`

归档和清理操作的入口

`do_install`

安装向导的入口

#### 配置说明

`./clean.conf`

用于配置组件即组件对应多个待归档目录的映射关系

```conf
# compName path1 path2

test1 /var/log/test1/comp1 /var/log/test1/comp2
test2 /var/log/test2
hdfs /var/log/hdfs ../log/yar
yarn /var/log/yarn
hadoop /var/log/hadoop
```

`./default.conf`

用于配置组件的归档方式，需要指定对应的正则表达式和执行的方法。

正则表达式用于 `find` 方法匹配路径，`action` 用于调用脚本内部的几个指令；目前只支持 `move`, `archive`, `delete`。

```conf
# 指令
#  move: 移动匹配的文件到当天的收集目录
#  archive: 打包被匹配的文件到当天收集目录下的对应组件的压缩档案
#  delete: 删除匹配的文件
#
# 表达式
#  正则表达式
#
# 例子
#  move *.tar.* *.tgz
#  archive *.log *.out
#  delete *.log

move *.tar.*
move *.tgz
archive *.log
archive *.log.*
archive *.log_*
archive *.out
delete *.log
delete *.out
delete *.log.*
delete *.log_*

```



